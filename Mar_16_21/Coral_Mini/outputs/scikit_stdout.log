Architecture:        aarch64
Byte Order:          Little Endian
CPU(s):              4
On-line CPU(s) list: 0-3
Thread(s) per core:  1
Core(s) per socket:  4
Socket(s):           1
Vendor ID:           ARM
Model:               1
Model name:          Cortex-A35
Stepping:            r0p1
CPU max MHz:         1300.0000
CPU min MHz:         598.0000
BogoMIPS:            26.00
Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid
MemTotal:        2044996 kB
================================================================================
================================================================================
Running run_forest_importances_faces test
perf stat -o ../output/run_forest_importances_faces.log --per-core -a taskset -c 0-3 ./run_forest_importances_faces.sh -n 4

=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.


=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.


=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.


=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.

/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
done in 12.453s
done in 12.460s
done in 12.496s
done in 12.508s
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 17:02:05 2021


 Performance counter stats for 'system wide':

S0-C0           1           27291.96 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              15517      context-switches          #    0.569 K/sec                  
S0-C0           1               1133      cpu-migrations            #    0.042 K/sec                  
S0-C0           1              40723      page-faults               #    0.001 M/sec                  
S0-C0           1         3957755183      cycles                    #    0.145 GHz                    
S0-C0           1         2832071518      instructions              #    0.72  insn per cycle         
S0-C0           1          795825976      branches                  #   29.160 M/sec                  
S0-C0           1          198044417      branch-misses             #   24.89% of all branches        
S0-C1           1           27291.96 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1              13454      context-switches          #    0.493 K/sec                  
S0-C1           1                891      cpu-migrations            #    0.033 K/sec                  
S0-C1           1              42837      page-faults               #    0.002 M/sec                  
S0-C1           1         4162226569      cycles                    #    0.153 GHz                    
S0-C1           1         2934369817      instructions              #    0.71  insn per cycle         
S0-C1           1          804816414      branches                  #   29.489 M/sec                  
S0-C1           1          200524900      branch-misses             #   24.92% of all branches        
S0-C2           1           27291.99 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              12811      context-switches          #    0.469 K/sec                  
S0-C2           1                768      cpu-migrations            #    0.028 K/sec                  
S0-C2           1              42599      page-faults               #    0.002 M/sec                  
S0-C2           1         4192459818      cycles                    #    0.154 GHz                    
S0-C2           1         2904790327      instructions              #    0.69  insn per cycle         
S0-C2           1          802305181      branches                  #   29.397 M/sec                  
S0-C2           1          199685312      branch-misses             #   24.89% of all branches        
S0-C3           1           27292.00 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1              13485      context-switches          #    0.494 K/sec                  
S0-C3           1                704      cpu-migrations            #    0.026 K/sec                  
S0-C3           1              43047      page-faults               #    0.002 M/sec                  
S0-C3           1         4119592060      cycles                    #    0.151 GHz                    
S0-C3           1         2912120262      instructions              #    0.71  insn per cycle         
S0-C3           1          802866830      branches                  #   29.418 M/sec                  
S0-C3           1          200364305      branch-misses             #   24.96% of all branches        

      27.292777791 seconds time elapsed

================================================================================
Running run_multioutput_face_completion test
perf stat -o ../output/run_multioutput_face_completion.log --per-core -a taskset -c 0-3 ./run_multioutput_face_completion.sh -n 4

==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.




==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.



==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 17:02:32 2021


 Performance counter stats for 'system wide':

S0-C0           1           51593.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1               6420      context-switches          #    0.124 K/sec                  
S0-C0           1                196      cpu-migrations            #    0.004 K/sec                  
S0-C0           1             120548      page-faults               #    0.002 M/sec                  
S0-C0           1         2399302339      cycles                    #    0.047 GHz                    
S0-C0           1         3410077221      instructions              #    1.42  insn per cycle         
S0-C0           1         1532680321      branches                  #   29.707 M/sec                  
S0-C0           1          133495542      branch-misses             #    8.71% of all branches        
S0-C1           1           51593.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               4802      context-switches          #    0.093 K/sec                  
S0-C1           1                214      cpu-migrations            #    0.004 K/sec                  
S0-C1           1             119070      page-faults               #    0.002 M/sec                  
S0-C1           1         2341451861      cycles                    #    0.045 GHz                    
S0-C1           1         3469492332      instructions              #    1.48  insn per cycle         
S0-C1           1         1529630576      branches                  #   29.648 M/sec                  
S0-C1           1          132061542      branch-misses             #    8.63% of all branches        
S0-C2           1           51593.48 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1               5888      context-switches          #    0.114 K/sec                  
S0-C2           1                127      cpu-migrations            #    0.002 K/sec                  
S0-C2           1             125041      page-faults               #    0.002 M/sec                  
S0-C2           1         2561690475      cycles                    #    0.050 GHz                    
S0-C2           1         3630190274      instructions              #    1.42  insn per cycle         
S0-C2           1         1536782491      branches                  #   29.786 M/sec                  
S0-C2           1          133397275      branch-misses             #    8.68% of all branches        
S0-C3           1           51593.48 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               5997      context-switches          #    0.116 K/sec                  
S0-C3           1                178      cpu-migrations            #    0.003 K/sec                  
S0-C3           1             124718      page-faults               #    0.002 M/sec                  
S0-C3           1         2317050135      cycles                    #    0.045 GHz                    
S0-C3           1         3482420769      instructions              #    1.50  insn per cycle         
S0-C3           1         1530687562      branches                  #   29.668 M/sec                  
S0-C3           1          132503488      branch-misses             #    8.66% of all branches        

      51.594470704 seconds time elapsed

================================================================================
Running run_logistic_path test
perf stat -o ../output/run_logistic_path.log --per-core -a taskset -c 0-3 ./run_logistic_path.sh -n 4

==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.


==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.


==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.





==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.


/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Computing regularization path ...
Computing regularization path ...
Computing regularization path ...
Computing regularization path ...
This took 33.491s
This took 33.880s
This took 35.351s
This took 35.672s
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 17:03:24 2021


 Performance counter stats for 'system wide':

S0-C0           1           44317.25 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1               9921      context-switches          #    0.224 K/sec                  
S0-C0           1                170      cpu-migrations            #    0.004 K/sec                  
S0-C0           1              30056      page-faults               #    0.678 K/sec                  
S0-C0           1         1026601490      cycles                    #    0.023 GHz                    
S0-C0           1          931182743      instructions              #    0.91  insn per cycle         
S0-C0           1         2750446062      branches                  #   62.063 M/sec                  
S0-C0           1          509772963      branch-misses             #   18.53% of all branches        
S0-C1           1           44317.25 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               5218      context-switches          #    0.118 K/sec                  
S0-C1           1                152      cpu-migrations            #    0.003 K/sec                  
S0-C1           1              30327      page-faults               #    0.684 K/sec                  
S0-C1           1          427661914      cycles                    #    0.010 GHz                    
S0-C1           1          525219880      instructions              #    1.23  insn per cycle         
S0-C1           1         2707042245      branches                  #   61.083 M/sec                  
S0-C1           1          452566782      branch-misses             #   16.72% of all branches        
S0-C2           1           44317.25 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1               6243      context-switches          #    0.141 K/sec                  
S0-C2           1                146      cpu-migrations            #    0.003 K/sec                  
S0-C2           1              23533      page-faults               #    0.531 K/sec                  
S0-C2           1         4052350707      cycles                    #    0.091 GHz                    
S0-C2           1         3952687460      instructions              #    0.98  insn per cycle         
S0-C2           1         2602624189      branches                  #   58.727 M/sec                  
S0-C2           1          485445324      branch-misses             #   18.65% of all branches        
S0-C3           1           44317.25 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               5342      context-switches          #    0.121 K/sec                  
S0-C3           1                160      cpu-migrations            #    0.004 K/sec                  
S0-C3           1              23367      page-faults               #    0.527 K/sec                  
S0-C3           1         4275477410      cycles                    #    0.096 GHz                    
S0-C3           1            3262484      instructions              #    0.00  insn per cycle         
S0-C3           1         2636085716      branches                  #   59.482 M/sec                  
S0-C3           1          503444850      branch-misses             #   19.10% of all branches        

      44.318075319 seconds time elapsed

================================================================================
Running run_plot_svm_nonlinear test
perf stat -o ../output/run_plot_svm_nonlinear.log --per-core -a taskset -c 0-3 ./run_plot_svm_nonlinear.sh -n 4

==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.


==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.


==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.


==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.

/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 17:04:08 2021


 Performance counter stats for 'system wide':

S0-C0           1           16036.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1               2666      context-switches          #    0.166 K/sec                  
S0-C0           1                103      cpu-migrations            #    0.006 K/sec                  
S0-C0           1              31282      page-faults               #    0.002 M/sec                  
S0-C0           1         3498811010      cycles                    #    0.218 GHz                    
S0-C0           1          394224267      instructions              #    0.11  insn per cycle         
S0-C0           1          941431696      branches                  #   58.706 M/sec                  
S0-C0           1           89142846      branch-misses             #    9.47% of all branches        
S0-C1           1           16036.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               1916      context-switches          #    0.119 K/sec                  
S0-C1           1                 89      cpu-migrations            #    0.006 K/sec                  
S0-C1           1              32564      page-faults               #    0.002 M/sec                  
S0-C1           1         3641431718      cycles                    #    0.227 GHz                    
S0-C1           1          482814809      instructions              #    0.13  insn per cycle         
S0-C1           1          951783569      branches                  #   59.351 M/sec                  
S0-C1           1           91292877      branch-misses             #    9.59% of all branches        
S0-C2           1           16036.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1               2154      context-switches          #    0.134 K/sec                  
S0-C2           1                115      cpu-migrations            #    0.007 K/sec                  
S0-C2           1              29283      page-faults               #    0.002 M/sec                  
S0-C2           1         3341903127      cycles                    #    0.208 GHz                    
S0-C2           1          415814774      instructions              #    0.12  insn per cycle         
S0-C2           1          944765197      branches                  #   58.913 M/sec                  
S0-C2           1           89006149      branch-misses             #    9.42% of all branches        
S0-C3           1           16036.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               2418      context-switches          #    0.151 K/sec                  
S0-C3           1                 64      cpu-migrations            #    0.004 K/sec                  
S0-C3           1              31346      page-faults               #    0.002 M/sec                  
S0-C3           1         3297487377      cycles                    #    0.206 GHz                    
S0-C3           1          458464368      instructions              #    0.14  insn per cycle         
S0-C3           1          949253685      branches                  #   59.193 M/sec                  
S0-C3           1           90351684      branch-misses             #    9.52% of all branches        

      16.036987579 seconds time elapsed

================================================================================
Running run_plot_theilsen test
perf stat -o ../output/run_plot_theilsen.log --per-core -a taskset -c 0-3 ./run_plot_theilsen.sh -n 4
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp

====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.

/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 17:04:25 2021


 Performance counter stats for 'system wide':

S0-C0           1           37406.20 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              17491      context-switches          #    0.468 K/sec                  
S0-C0           1                118      cpu-migrations            #    0.003 K/sec                  
S0-C0           1              43774      page-faults               #    0.001 M/sec                  
S0-C0           1          883868282      cycles                    #    0.024 GHz                    
S0-C0           1         1377266240      instructions              #    1.56  insn per cycle         
S0-C0           1         1178102014      branches                  #   31.495 M/sec                  
S0-C0           1          344545509      branch-misses             #   29.25% of all branches        
S0-C1           1           37406.20 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               4735      context-switches          #    0.127 K/sec                  
S0-C1           1                128      cpu-migrations            #    0.003 K/sec                  
S0-C1           1              46717      page-faults               #    0.001 M/sec                  
S0-C1           1          776487174      cycles                    #    0.021 GHz                    
S0-C1           1         1394100358      instructions              #    1.80  insn per cycle         
S0-C1           1         1179745030      branches                  #   31.539 M/sec                  
S0-C1           1          343286449      branch-misses             #   29.10% of all branches        
S0-C2           1           37406.20 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1               5442      context-switches          #    0.145 K/sec                  
S0-C2           1                113      cpu-migrations            #    0.003 K/sec                  
S0-C2           1              46966      page-faults               #    0.001 M/sec                  
S0-C2           1          841374870      cycles                    #    0.022 GHz                    
S0-C2           1         1389141900      instructions              #    1.65  insn per cycle         
S0-C2           1         1179334743      branches                  #   31.528 M/sec                  
S0-C2           1          340088560      branch-misses             #   28.84% of all branches        
S0-C3           1           37406.20 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               3177      context-switches          #    0.085 K/sec                  
S0-C3           1                102      cpu-migrations            #    0.003 K/sec                  
S0-C3           1              50262      page-faults               #    0.001 M/sec                  
S0-C3           1         1346238581      cycles                    #    0.036 GHz                    
S0-C3           1         1550090949      instructions              #    1.15  insn per cycle         
S0-C3           1         1201188661      branches                  #   32.112 M/sec                  
S0-C3           1          345495060      branch-misses             #   28.76% of all branches        

      37.406906974 seconds time elapsed

================================================================================
================================================================================
Printing results
File: run_forest_importances_faces.txt
instructions: 11583351924
=(2832071518+2934369817+2904790327+2912120262)
frequencies: 0.151
=(0.145+0.153+0.154+0.151)/4
ipcs:0.71
=1*(0.72+0.71+0.69+0.71)/4

File: run_plot_svm_nonlinear.txt
instructions: 1751318218
=(394224267+482814809+415814774+458464368)
frequencies: 0.215
=(0.218+0.227+0.208+0.206)/4
ipcs:0.12
=1*(0.11+0.13+0.12+0.14)/4

File: run_multioutput_face_completion.txt
instructions: 13992180596
=(3410077221+3469492332+3630190274+3482420769)
frequencies: 0.047
=(0.047+0.045+0.050+0.045)/4
ipcs:1.46
=1*(1.42+1.48+1.42+1.50)/4

File: run_plot_theilsen.txt
instructions: 5710599447
=(1377266240+1394100358+1389141900+1550090949)
frequencies: 0.026
=(0.024+0.021+0.022+0.036)/4
ipcs:1.54
=1*(1.56+1.80+1.65+1.15)/4

================================================================================
Finished running benchmarks
================================================================================

