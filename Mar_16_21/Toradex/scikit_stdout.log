Architecture:        aarch64
Byte Order:          Little Endian
CPU(s):              4
On-line CPU(s) list: 0-3
Thread(s) per core:  1
Core(s) per socket:  4
Socket(s):           1
NUMA node(s):        1
Vendor ID:           ARM
Model:               2
Model name:          Cortex-A35
Stepping:            r0p2
CPU max MHz:         1200.0000
CPU min MHz:         900.0000
BogoMIPS:            16.00
L1d cache:           unknown size
L1i cache:           unknown size
L2 cache:            unknown size
NUMA node0 CPU(s):   0-3
Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid
MemTotal:        1818364 kB
================================================================================
================================================================================
Running run_forest_importances_faces test
perf stat -o ../output/run_forest_importances_faces.log --per-core -a taskset -c 0-3 ./run_forest_importances_faces.sh -n 4

=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.

=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.



=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.


=================================================
Pixel importances with a parallel forest of trees
=================================================

This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.

The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.

/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
Fitting ExtraTreesClassifier on faces data with 1 cores...
done in 12.011s
done in 12.007s
done in 12.021s
done in 12.078s
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 19:53:51 2021


 Performance counter stats for 'system wide':

S0-C0           1           24061.83 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              16755      context-switches          #    0.696 K/sec                  
S0-C0           1                861      cpu-migrations            #    0.036 K/sec                  
S0-C0           1              36581      page-faults               #    0.002 M/sec                  
S0-C0           1        26838540600      cycles                    #    1.115 GHz                    
S0-C0           1         7018525597      instructions              #    0.26  insn per cycle         
S0-C0           1          782765743      branches                  #   32.531 M/sec                  
S0-C0           1          195944383      branch-misses             #   25.03% of all branches        
S0-C1           1           24061.84 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1              15098      context-switches          #    0.627 K/sec                  
S0-C1           1                642      cpu-migrations            #    0.027 K/sec                  
S0-C1           1              38226      page-faults               #    0.002 M/sec                  
S0-C1           1        26956243389      cycles                    #    1.120 GHz                    
S0-C1           1         7102234884      instructions              #    0.26  insn per cycle         
S0-C1           1          789530746      branches                  #   32.813 M/sec                  
S0-C1           1          197922928      branch-misses             #   25.07% of all branches        
S0-C2           1           24061.85 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              12968      context-switches          #    0.539 K/sec                  
S0-C2           1                516      cpu-migrations            #    0.021 K/sec                  
S0-C2           1              39057      page-faults               #    0.002 M/sec                  
S0-C2           1        27022932492      cycles                    #    1.123 GHz                    
S0-C2           1         7075660375      instructions              #    0.26  insn per cycle         
S0-C2           1          787950125      branches                  #   32.747 M/sec                  
S0-C2           1          197168077      branch-misses             #   25.02% of all branches        
S0-C3           1           24061.88 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1              13008      context-switches          #    0.541 K/sec                  
S0-C3           1                348      cpu-migrations            #    0.014 K/sec                  
S0-C3           1              40442      page-faults               #    0.002 M/sec                  
S0-C3           1        27021027263      cycles                    #    1.123 GHz                    
S0-C3           1         7069218175      instructions              #    0.26  insn per cycle         
S0-C3           1          786677160      branches                  #   32.694 M/sec                  
S0-C3           1          197214072      branch-misses             #   25.07% of all branches        

      24.064974752 seconds time elapsed

================================================================================
Running run_multioutput_face_completion test
perf stat -o ../output/run_multioutput_face_completion.log --per-core -a taskset -c 0-3 ./run_multioutput_face_completion.sh -n 4

==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.





==============================================
Face completion with a multi-output estimators
==============================================

This example shows the use of multi-output estimator to complete images.
The goal is to predict the lower half of a face given its upper half.

The first column of images shows true faces. The next columns illustrate
how extremely randomized trees, k nearest neighbors, linear
regression and ridge regression complete the lower half of those faces.


/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 19:54:16 2021


 Performance counter stats for 'system wide':

S0-C0           1           52514.10 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1               7717      context-switches          #    0.147 K/sec                  
S0-C0           1                185      cpu-migrations            #    0.004 K/sec                  
S0-C0           1              50622      page-faults               #    0.964 K/sec                  
S0-C0           1        62734297178      cycles                    #    1.195 GHz                    
S0-C0           1        24522059333      instructions              #    0.39  insn per cycle         
S0-C0           1         1476899977      branches                  #   28.124 M/sec                  
S0-C0           1          129478509      branch-misses             #    8.77% of all branches        
S0-C1           1           52514.10 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               6532      context-switches          #    0.124 K/sec                  
S0-C1           1                155      cpu-migrations            #    0.003 K/sec                  
S0-C1           1              51105      page-faults               #    0.973 K/sec                  
S0-C1           1        62679216259      cycles                    #    1.194 GHz                    
S0-C1           1        24538688981      instructions              #    0.39  insn per cycle         
S0-C1           1         1477582874      branches                  #   28.137 M/sec                  
S0-C1           1          128976829      branch-misses             #    8.73% of all branches        
S0-C2           1           52514.10 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1               8688      context-switches          #    0.165 K/sec                  
S0-C2           1                145      cpu-migrations            #    0.003 K/sec                  
S0-C2           1              53812      page-faults               #    0.001 M/sec                  
S0-C2           1        62905184645      cycles                    #    1.198 GHz                    
S0-C2           1        24560724525      instructions              #    0.39  insn per cycle         
S0-C2           1         1481487409      branches                  #   28.211 M/sec                  
S0-C2           1          129715711      branch-misses             #    8.76% of all branches        
S0-C3           1           52514.10 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               7041      context-switches          #    0.134 K/sec                  
S0-C3           1                121      cpu-migrations            #    0.002 K/sec                  
S0-C3           1              53091      page-faults               #    0.001 M/sec                  
S0-C3           1        62694601095      cycles                    #    1.194 GHz                    
S0-C3           1        24496955393      instructions              #    0.39  insn per cycle         
S0-C3           1         1473484439      branches                  #   28.059 M/sec                  
S0-C3           1          129107379      branch-misses             #    8.76% of all branches        

      52.519769741 seconds time elapsed

================================================================================
Running run_logistic_path test
perf stat -o ../output/run_logistic_path.log --per-core -a taskset -c 0-3 ./run_logistic_path.sh -n 4

==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.


==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.



==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.



==============================================
Regularization path of L1- Logistic Regression
==============================================


Train l1-penalized logistic regression models on a binary classification
problem derived from the Iris dataset.

The models are ordered from strongest regularized to least regularized. The 4
coefficients of the models are collected and plotted as a "regularization
path": on the left-hand side of the figure (strong regularizers), all the
coefficients are exactly 0. When regularization gets progressively looser,
coefficients can get non-zero values one after the other.

Here we choose the SAGA solver because it can efficiently optimize for the
Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

Also note that we set a low value for the tolerance to make sure that the model
has converged before collecting the coefficients.

We also use warm_start=True which means that the coefficients of the models are
reused to initialize the next model fit to speed-up the computation of the
full-path.



/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Computing regularization path ...
Computing regularization path ...
Computing regularization path ...
Computing regularization path ...
This took 36.441s
This took 37.926s
This took 38.545s
This took 38.889s
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 19:55:08 2021


 Performance counter stats for 'system wide':

S0-C0           1           47199.47 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1              10685      context-switches          #    0.226 K/sec                  
S0-C0           1                205      cpu-migrations            #    0.004 K/sec                  
S0-C0           1              29332      page-faults               #    0.621 K/sec                  
S0-C0           1        56279710443      cycles                    #    1.192 GHz                    
S0-C0           1        26344792418      instructions              #    0.47  insn per cycle         
S0-C0           1         2711164263      branches                  #   57.441 M/sec                  
S0-C0           1          478803278      branch-misses             #   17.66% of all branches        
S0-C1           1           47199.46 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               6932      context-switches          #    0.147 K/sec                  
S0-C1           1                184      cpu-migrations            #    0.004 K/sec                  
S0-C1           1              23598      page-faults               #    0.500 K/sec                  
S0-C1           1        53785339562      cycles                    #    1.140 GHz                    
S0-C1           1        25749096884      instructions              #    0.48  insn per cycle         
S0-C1           1         2634808984      branches                  #   55.823 M/sec                  
S0-C1           1          466654552      branch-misses             #   17.71% of all branches        
S0-C2           1           47199.48 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1               8336      context-switches          #    0.177 K/sec                  
S0-C2           1                167      cpu-migrations            #    0.004 K/sec                  
S0-C2           1              26283      page-faults               #    0.557 K/sec                  
S0-C2           1        55415327255      cycles                    #    1.174 GHz                    
S0-C2           1        26032841608      instructions              #    0.47  insn per cycle         
S0-C2           1         2668798497      branches                  #   56.543 M/sec                  
S0-C2           1          447015858      branch-misses             #   16.75% of all branches        
S0-C3           1           47199.49 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               9109      context-switches          #    0.193 K/sec                  
S0-C3           1                161      cpu-migrations            #    0.003 K/sec                  
S0-C3           1              26600      page-faults               #    0.564 K/sec                  
S0-C3           1        56563330519      cycles                    #    1.198 GHz                    
S0-C3           1        26025867363      instructions              #    0.46  insn per cycle         
S0-C3           1         2669322358      branches                  #   56.554 M/sec                  
S0-C3           1          499653022      branch-misses             #   18.72% of all branches        

      47.204273332 seconds time elapsed

================================================================================
Running run_plot_svm_nonlinear test
perf stat -o ../output/run_plot_svm_nonlinear.log --per-core -a taskset -c 0-3 ./run_plot_svm_nonlinear.sh -n 4

==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.

==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.

==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.




==============
Non-linear SVM
==============

Perform binary classification using non-linear SVC
with RBF kernel. The target to predict is a XOR of the
inputs.

The color map illustrates the decision function learned by the SVC.

/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 19:55:55 2021


 Performance counter stats for 'system wide':

S0-C0           1           16310.44 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1               2746      context-switches          #    0.168 K/sec                  
S0-C0           1                160      cpu-migrations            #    0.010 K/sec                  
S0-C0           1              27744      page-faults               #    0.002 M/sec                  
S0-C0           1        19344654896      cycles                    #    1.186 GHz                    
S0-C0           1         8948029580      instructions              #    0.46  insn per cycle         
S0-C0           1          937294958      branches                  #   57.466 M/sec                  
S0-C0           1           87787445      branch-misses             #    9.37% of all branches        
S0-C1           1           16310.44 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               3709      context-switches          #    0.227 K/sec                  
S0-C1           1                131      cpu-migrations            #    0.008 K/sec                  
S0-C1           1              30106      page-faults               #    0.002 M/sec                  
S0-C1           1        19413394762      cycles                    #    1.190 GHz                    
S0-C1           1         8972661891      instructions              #    0.46  insn per cycle         
S0-C1           1          939418920      branches                  #   57.596 M/sec                  
S0-C1           1           89624998      branch-misses             #    9.54% of all branches        
S0-C2           1           16310.44 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1               3645      context-switches          #    0.223 K/sec                  
S0-C2           1                119      cpu-migrations            #    0.007 K/sec                  
S0-C2           1              29505      page-faults               #    0.002 M/sec                  
S0-C2           1        19555872620      cycles                    #    1.199 GHz                    
S0-C2           1         8990338160      instructions              #    0.46  insn per cycle         
S0-C2           1          941624504      branches                  #   57.731 M/sec                  
S0-C2           1           89605872      branch-misses             #    9.52% of all branches        
S0-C3           1           16310.44 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               2500      context-switches          #    0.153 K/sec                  
S0-C3           1                101      cpu-migrations            #    0.006 K/sec                  
S0-C3           1              30207      page-faults               #    0.002 M/sec                  
S0-C3           1        19373408159      cycles                    #    1.188 GHz                    
S0-C3           1         8971004891      instructions              #    0.46  insn per cycle         
S0-C3           1          939915069      branches                  #   57.627 M/sec                  
S0-C3           1           88828540      branch-misses             #    9.45% of all branches        

      16.312233729 seconds time elapsed

================================================================================
Running run_plot_theilsen test
perf stat -o ../output/run_plot_theilsen.log --per-core -a taskset -c 0-3 ./run_plot_theilsen.sh -n 4
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/lib/python3/dist-packages/sklearn/externals/joblib.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp

====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.


====================
Theil-Sen Regression
====================

Computes a Theil-Sen Regression on a synthetic dataset.

See :ref:`theil_sen_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.

The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.

In certain cases Theil-Sen performs better than :ref:`RANSAC
<ransac_regression>` which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the ``residual_threshold`` parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the ``max_subpopulation`` parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.

/root/i-benchmarks/scikit/bin
# started on Tue Mar 16 19:56:12 2021


 Performance counter stats for 'system wide':

S0-C0           1           32279.39 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C0           1               8695      context-switches          #    0.269 K/sec                  
S0-C0           1                151      cpu-migrations            #    0.005 K/sec                  
S0-C0           1              46557      page-faults               #    0.001 M/sec                  
S0-C0           1        38690129186      cycles                    #    1.199 GHz                    
S0-C0           1         9983339397      instructions              #    0.26  insn per cycle         
S0-C0           1         1178866392      branches                  #   36.521 M/sec                  
S0-C0           1          344231443      branch-misses             #   29.20% of all branches        
S0-C1           1           32279.39 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C1           1               5728      context-switches          #    0.177 K/sec                  
S0-C1           1                131      cpu-migrations            #    0.004 K/sec                  
S0-C1           1              46359      page-faults               #    0.001 M/sec                  
S0-C1           1        38693874748      cycles                    #    1.199 GHz                    
S0-C1           1         9979949713      instructions              #    0.26  insn per cycle         
S0-C1           1         1176585912      branches                  #   36.450 M/sec                  
S0-C1           1          339707686      branch-misses             #   28.87% of all branches        
S0-C2           1           32279.39 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C2           1              15889      context-switches          #    0.492 K/sec                  
S0-C2           1                132      cpu-migrations            #    0.004 K/sec                  
S0-C2           1              47044      page-faults               #    0.001 M/sec                  
S0-C2           1        38700201703      cycles                    #    1.199 GHz                    
S0-C2           1         9919965603      instructions              #    0.26  insn per cycle         
S0-C2           1         1172259669      branches                  #   36.316 M/sec                  
S0-C2           1          339750467      branch-misses             #   28.98% of all branches        
S0-C3           1           32279.39 msec cpu-clock                 #    1.000 CPUs utilized          
S0-C3           1               3480      context-switches          #    0.108 K/sec                  
S0-C3           1                 92      cpu-migrations            #    0.003 K/sec                  
S0-C3           1              46957      page-faults               #    0.001 M/sec                  
S0-C3           1        38702918839      cycles                    #    1.199 GHz                    
S0-C3           1         9909274912      instructions              #    0.26  insn per cycle         
S0-C3           1         1171263328      branches                  #   36.285 M/sec                  
S0-C3           1          340882726      branch-misses             #   29.10% of all branches        

      32.282507108 seconds time elapsed

================================================================================
================================================================================
Printing results
File: run_forest_importances_faces.txt
instructions: 28265639031
=(7018525597+7102234884+7075660375+7069218175)
frequencies: 1.120
=(1.115+1.120+1.123+1.123)/4
ipcs:0.26
=1*(0.26+0.26+0.26+0.26)/4

File: run_plot_svm_nonlinear.txt
instructions: 35882034522
=(8948029580+8972661891+8990338160+8971004891)
frequencies: 1.191
=(1.186+1.190+1.199+1.188)/4
ipcs:0.46
=1*(0.46+0.46+0.46+0.46)/4

File: run_multioutput_face_completion.txt
instructions: 98118428232
=(24522059333+24538688981+24560724525+24496955393)
frequencies: 1.195
=(1.195+1.194+1.198+1.194)/4
ipcs:0.39
=1*(0.39+0.39+0.39+0.39)/4

File: run_plot_theilsen.txt
instructions: 39792529625
=(9983339397+9979949713+9919965603+9909274912)
frequencies: 1.199
=(1.199+1.199+1.199+1.199)/4
ipcs:0.26
=1*(0.26+0.26+0.26+0.26)/4

================================================================================
Finished running benchmarks
================================================================================
